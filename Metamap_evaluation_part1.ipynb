{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h1>Metamap Evaluation tool V1 - part 1</h1></b>\n",
    "This tool will help to evaluate and compare different metamap behavioural options.\n",
    "Part 1- This will have the code of reading the text and building phrase and word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Requirements</h2>\n",
    "<ol>\n",
    "<li>Create the conda environment with all the libraries. Local installation of Metamap.</li>\n",
    "<li>Installing pubmed edirect (https://www.ncbi.nlm.nih.gov/books/NBK179288/)</li>\n",
    "<li>Installing Metamap on linux locally (https://metamap.nlm.nih.gov/Docs/README.html)</li>\n",
    "<li>Build pubmed parser from source code, as we did some changes to the standard version. Its provided in repository</li>\n",
    "<li>Papers text from pubmed</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 1: Getting the text and building word2vec model</h3>\n",
    "Getting the paper abstracts, preprocessing, forming phrase and training word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a - reading paper abstracts\n",
    "\n",
    "\n",
    "import pubmed_parser as pp\n",
    "\n",
    "dicts_out = pp.parse_medline_xml('/home/aindani/CDSS/vectorization/processed_main_script_out_1_5_2021.xml',\n",
    "                                 year_info_only=False,\n",
    "                                 nlm_category=False,\n",
    "                                 author_list=False,\n",
    "                                 reference_list=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:58:48: loading projection weights from /home/aindani/gensim-data/glove-twitter-25/glove-twitter-25.gz\n",
      "INFO - 11:59:08: KeyedVectors lifecycle event {'msg': 'loaded (1193514, 25) matrix of type float32 from /home/aindani/gensim-data/glove-twitter-25/glove-twitter-25.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-07-09T11:59:08.702764', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "#1b preprocessing\n",
    "\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "import json\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Choose model accordingly for contractions function\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "# model = api.load(\"glove-twitter-100\")\n",
    "# model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "cont = Contractions(kv_model=model)\n",
    "cont.load_models()\n",
    "\n",
    "# exclude words from spacy stopwords list\n",
    "deselect_stop_words = ['no', 'not']\n",
    "for w in deselect_stop_words:\n",
    "    nlp.vocab[w].is_stop = False\n",
    "    \n",
    "\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    \"\"\"remove extra whitespaces from text\"\"\"\n",
    "    text = text.strip()\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"remove accented characters from text, e.g. cafÃ© --> cafe\"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text, accented_chars=True, contractions=True, \n",
    "                       convert_num=True, extra_whitespace=True, \n",
    "                       lemmatization=True, lowercase=True, punctuations=True, \n",
    "                       remove_num=True, special_chars=True, \n",
    "                       stop_words=True):\n",
    "    \"\"\"preprocess text with default option set to true for all steps\"\"\"\n",
    "   \n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        text = remove_whitespace(text)\n",
    "    if accented_chars == True: #remove accented characters\n",
    "        text = remove_accented_chars(text)\n",
    "    if contractions == True: #expand contractions\n",
    "        text = expand_contractions(text)\n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "    doc = nlp(text) #tokenise text\n",
    "    #print(doc)\n",
    "    clean_text = []\n",
    "    \n",
    "    #remove_unwanted_dict={'objective'}\n",
    "    \n",
    "    for token in doc:\n",
    "        #print(token)\n",
    "        flag = True\n",
    "        edit = token.text\n",
    "        '''\n",
    "        #remove unwanted like objective, methods\n",
    "        if token.text in remove_unwanted_dict:\n",
    "            flag = False\n",
    "        '''\n",
    "        \n",
    "        # remove stop words\n",
    "        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n",
    "            flag = False\n",
    "        # remove punctuations\n",
    "        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n",
    "            flag = False\n",
    "        # remove special characters\n",
    "        if special_chars == True and token.pos_ == 'SYM' and flag == True: \n",
    "            flag = False\n",
    "        # remove numbers\n",
    "        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n",
    "        and flag == True:\n",
    "            flag = False\n",
    "        # convert number words to numeric numbers\n",
    "        if convert_num == True and token.pos_ == 'NUM' and flag == True:\n",
    "            edit = w2n.word_to_num(token.text)\n",
    "        # convert tokens to base form\n",
    "        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n",
    "            edit = token.lemma_\n",
    "        # append tokens edited and not removed to list \n",
    "        if edit != \"\" and flag == True:\n",
    "            clean_text.append(edit)        \n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1c function to form phrases\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "def build_phrases(sentences):\n",
    "    phrases = Phrases(sentences,\n",
    "                      min_count=3,\n",
    "                      threshold=0.5,\n",
    "                      scoring='npmi',\n",
    "                      progress_per=1000)\n",
    "    return Phraser(phrases)\n",
    "#title handling giving more weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:59:30: Removed 38 and 38 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 11:59:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:59:30: built Dictionary(83 unique tokens: ['a', 'advice', 'along', 'an', 'and']...) from 2 documents (total 303 corpus positions)\n",
      "INFO - 11:59:30: Dictionary lifecycle event {'msg': \"built Dictionary(83 unique tokens: ['a', 'advice', 'along', 'an', 'and']...) from 2 documents (total 303 corpus positions)\", 'datetime': '2021-07-09T11:59:30.546742', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 11:59:31: Removed 38 and 38 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 11:59:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:59:31: built Dictionary(83 unique tokens: ['a', 'advice', 'along', 'an', 'and']...) from 2 documents (total 303 corpus positions)\n",
      "INFO - 11:59:31: Dictionary lifecycle event {'msg': \"built Dictionary(83 unique tokens: ['a', 'advice', 'along', 'an', 'and']...) from 2 documents (total 303 corpus positions)\", 'datetime': '2021-07-09T11:59:31.832398', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 11:59:36: Removed 31 and 30 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 11:59:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:59:36: built Dictionary(78 unique tokens: ['a', 'accordingly', 'accurate', 'aim', 'among']...) from 2 documents (total 222 corpus positions)\n",
      "INFO - 11:59:36: Dictionary lifecycle event {'msg': \"built Dictionary(78 unique tokens: ['a', 'accordingly', 'accurate', 'aim', 'among']...) from 2 documents (total 222 corpus positions)\", 'datetime': '2021-07-09T11:59:36.667669', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 11:59:36: Removed 31 and 30 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 11:59:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:59:36: built Dictionary(78 unique tokens: ['a', 'accordingly', 'accurate', 'aim', 'among']...) from 2 documents (total 222 corpus positions)\n",
      "INFO - 11:59:36: Dictionary lifecycle event {'msg': \"built Dictionary(78 unique tokens: ['a', 'accordingly', 'accurate', 'aim', 'among']...) from 2 documents (total 222 corpus positions)\", 'datetime': '2021-07-09T11:59:36.764107', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 11:59:38: Removed 87 and 86 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 11:59:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:59:38: built Dictionary(92 unique tokens: ['a', 'according', 'active', 'additional', 'adherence']...) from 2 documents (total 326 corpus positions)\n",
      "INFO - 11:59:38: Dictionary lifecycle event {'msg': \"built Dictionary(92 unique tokens: ['a', 'according', 'active', 'additional', 'adherence']...) from 2 documents (total 326 corpus positions)\", 'datetime': '2021-07-09T11:59:38.277215', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 11:59:38: Removed 87 and 86 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 11:59:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:59:38: built Dictionary(92 unique tokens: ['a', 'according', 'active', 'additional', 'adherence']...) from 2 documents (total 326 corpus positions)\n",
      "INFO - 11:59:38: Dictionary lifecycle event {'msg': \"built Dictionary(92 unique tokens: ['a', 'according', 'active', 'additional', 'adherence']...) from 2 documents (total 326 corpus positions)\", 'datetime': '2021-07-09T11:59:38.461064', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 11:59:47: Removed 37 and 38 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 11:59:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:59:47: built Dictionary(66 unique tokens: ['a', 'and', 'applied', 'are', 'at']...) from 2 documents (total 202 corpus positions)\n",
      "INFO - 11:59:47: Dictionary lifecycle event {'msg': \"built Dictionary(66 unique tokens: ['a', 'and', 'applied', 'are', 'at']...) from 2 documents (total 202 corpus positions)\", 'datetime': '2021-07-09T11:59:47.773329', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 11:59:47: Removed 37 and 38 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 11:59:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 11:59:47: built Dictionary(66 unique tokens: ['a', 'and', 'applied', 'are', 'at']...) from 2 documents (total 202 corpus positions)\n",
      "INFO - 11:59:47: Dictionary lifecycle event {'msg': \"built Dictionary(66 unique tokens: ['a', 'and', 'applied', 'are', 'at']...) from 2 documents (total 202 corpus positions)\", 'datetime': '2021-07-09T11:59:47.871658', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:20: Removed 23 and 22 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:20: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:20: built Dictionary(69 unique tokens: ['a', 'access', 'activities', 'and', 'applied']...) from 2 documents (total 248 corpus positions)\n",
      "INFO - 12:00:20: Dictionary lifecycle event {'msg': \"built Dictionary(69 unique tokens: ['a', 'access', 'activities', 'and', 'applied']...) from 2 documents (total 248 corpus positions)\", 'datetime': '2021-07-09T12:00:20.959147', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:21: Removed 23 and 22 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:21: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:21: built Dictionary(70 unique tokens: ['a', 'access', 'activities', 'and', 'applied']...) from 2 documents (total 248 corpus positions)\n",
      "INFO - 12:00:21: Dictionary lifecycle event {'msg': \"built Dictionary(70 unique tokens: ['a', 'access', 'activities', 'and', 'applied']...) from 2 documents (total 248 corpus positions)\", 'datetime': '2021-07-09T12:00:21.043477', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:27: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:27: built Dictionary(40 unique tokens: ['a', 'already', 'an', 'and', 'are']...) from 2 documents (total 102 corpus positions)\n",
      "INFO - 12:00:27: Dictionary lifecycle event {'msg': \"built Dictionary(40 unique tokens: ['a', 'already', 'an', 'and', 'are']...) from 2 documents (total 102 corpus positions)\", 'datetime': '2021-07-09T12:00:27.312690', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:27: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:27: built Dictionary(41 unique tokens: ['a', 'already', 'an', 'and', 'are']...) from 2 documents (total 102 corpus positions)\n",
      "INFO - 12:00:27: Dictionary lifecycle event {'msg': \"built Dictionary(41 unique tokens: ['a', 'already', 'an', 'and', 'are']...) from 2 documents (total 102 corpus positions)\", 'datetime': '2021-07-09T12:00:27.350297', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:32: Removed 14 and 13 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:32: built Dictionary(47 unique tokens: ['a', 'and', 'as', 'disasters', 'drugs']...) from 2 documents (total 106 corpus positions)\n",
      "INFO - 12:00:32: Dictionary lifecycle event {'msg': \"built Dictionary(47 unique tokens: ['a', 'and', 'as', 'disasters', 'drugs']...) from 2 documents (total 106 corpus positions)\", 'datetime': '2021-07-09T12:00:32.039503', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:32: Removed 14 and 13 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:32: built Dictionary(47 unique tokens: ['a', 'and', 'as', 'disasters', 'drugs']...) from 2 documents (total 106 corpus positions)\n",
      "INFO - 12:00:32: Dictionary lifecycle event {'msg': \"built Dictionary(47 unique tokens: ['a', 'and', 'as', 'disasters', 'drugs']...) from 2 documents (total 106 corpus positions)\", 'datetime': '2021-07-09T12:00:32.086853', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:32: Removed 72 and 71 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:32: built Dictionary(131 unique tokens: ['a', 'activities', 'added', 'additional', 'adverse']...) from 2 documents (total 408 corpus positions)\n",
      "INFO - 12:00:32: Dictionary lifecycle event {'msg': \"built Dictionary(131 unique tokens: ['a', 'activities', 'added', 'additional', 'adverse']...) from 2 documents (total 408 corpus positions)\", 'datetime': '2021-07-09T12:00:32.941164', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:33: Removed 72 and 71 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:33: built Dictionary(131 unique tokens: ['a', 'activities', 'added', 'additional', 'adverse']...) from 2 documents (total 408 corpus positions)\n",
      "INFO - 12:00:33: Dictionary lifecycle event {'msg': \"built Dictionary(131 unique tokens: ['a', 'activities', 'added', 'additional', 'adverse']...) from 2 documents (total 408 corpus positions)\", 'datetime': '2021-07-09T12:00:33.149338', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:33: Removed 44 and 43 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:33: built Dictionary(106 unique tokens: ['.', 'a', 'an', 'and', 'any']...) from 2 documents (total 304 corpus positions)\n",
      "INFO - 12:00:33: Dictionary lifecycle event {'msg': \"built Dictionary(106 unique tokens: ['.', 'a', 'an', 'and', 'any']...) from 2 documents (total 304 corpus positions)\", 'datetime': '2021-07-09T12:00:33.693900', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:33: Removed 44 and 43 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 12:00:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 12:00:33: built Dictionary(106 unique tokens: ['.', 'a', 'an', 'and', 'any']...) from 2 documents (total 304 corpus positions)\n",
      "INFO - 12:00:33: Dictionary lifecycle event {'msg': \"built Dictionary(106 unique tokens: ['.', 'a', 'an', 'and', 'any']...) from 2 documents (total 304 corpus positions)\", 'datetime': '2021-07-09T12:00:33.847655', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "#looping over all the abstracts to do text preprocessing\n",
    "processed_abstracts=[]\n",
    "for doc in dicts_out:\n",
    "    clean_text=text_preprocessing(doc['abstract'])\n",
    "    processed_abstracts.append(clean_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:00:38: collecting all words and their counts\n",
      "INFO - 12:00:38: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 12:00:39: PROGRESS: at sentence #1000, processed 148403 words and 104220 word types\n",
      "INFO - 12:00:39: PROGRESS: at sentence #2000, processed 279048 words and 175557 word types\n",
      "INFO - 12:00:39: PROGRESS: at sentence #3000, processed 390948 words and 232127 word types\n",
      "INFO - 12:00:39: collected 241080 token types (unigram + bigrams) from a corpus of 409831 words and 3200 sentences\n",
      "INFO - 12:00:39: merged Phrases<241080 vocab, min_count=3, threshold=0.5, max_vocab_size=40000000>\n",
      "INFO - 12:00:39: Phrases lifecycle event {'msg': 'built Phrases<241080 vocab, min_count=3, threshold=0.5, max_vocab_size=40000000> in 0.42s', 'datetime': '2021-07-09T12:00:39.328074', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n",
      "INFO - 12:00:39: exporting phrases from Phrases<241080 vocab, min_count=3, threshold=0.5, max_vocab_size=40000000>\n",
      "INFO - 12:00:39: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2499 phrases, min_count=3, threshold=0.5> from Phrases<241080 vocab, min_count=3, threshold=0.5, max_vocab_size=40000000> in 0.34s', 'datetime': '2021-07-09T12:00:39.671854', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "ans=build_phrases(processed_abstracts[:3200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2: Training word2vec model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "all_abs_tokens_phrases=[]\n",
    "for abstract_tokens in processed_abstracts[:3200]:\n",
    "    all_abs_tokens_phrases.append(ans[abstract_tokens])\n",
    "\n",
    "\n",
    "#saving for future reference\n",
    "with open(\"all_abs_tokens_phrases.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(all_abs_tokens_phrases, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:03:42: collecting all words and their counts\n",
      "INFO - 12:03:42: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 12:03:42: collected 16502 word types from a corpus of 380879 raw words and 3200 sentences\n",
      "INFO - 12:03:42: Creating a fresh vocabulary\n",
      "INFO - 12:03:42: Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 16502 unique words (100.0%% of original 16502, drops 0)', 'datetime': '2021-07-09T12:03:42.800970', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'prepare_vocab'}\n",
      "INFO - 12:03:42: Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 380879 word corpus (100.0%% of original 380879, drops 0)', 'datetime': '2021-07-09T12:03:42.801400', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'prepare_vocab'}\n",
      "INFO - 12:03:42: deleting the raw counts dictionary of 16502 items\n",
      "INFO - 12:03:42: sample=0.001 downsamples 51 most-common words\n",
      "INFO - 12:03:42: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 349030.5171188597 word corpus (91.6%% of prior 380879)', 'datetime': '2021-07-09T12:03:42.866316', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'prepare_vocab'}\n",
      "INFO - 12:03:42: estimated required memory for 16502 words and 150 dimensions: 28053400 bytes\n",
      "INFO - 12:03:42: resetting layer weights\n",
      "INFO - 12:03:42: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-07-09T12:03:42.994744', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'build_vocab'}\n",
      "INFO - 12:03:42: Word2Vec lifecycle event {'msg': 'training model with 10 workers on 16502 vocabulary and 150 features, using sg=1 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2021-07-09T12:03:42.995360', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'train'}\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:43: EPOCH - 1 : training on 380879 raw words (348910 effective words) took 0.4s, 854036 effective words/s\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:43: EPOCH - 2 : training on 380879 raw words (349119 effective words) took 0.4s, 802401 effective words/s\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:44: EPOCH - 3 : training on 380879 raw words (348931 effective words) took 0.4s, 839335 effective words/s\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:44: EPOCH - 4 : training on 380879 raw words (348963 effective words) took 0.4s, 835613 effective words/s\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:45: EPOCH - 5 : training on 380879 raw words (348870 effective words) took 0.4s, 826960 effective words/s\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:45: EPOCH - 6 : training on 380879 raw words (348974 effective words) took 0.4s, 823652 effective words/s\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:45: EPOCH - 7 : training on 380879 raw words (348900 effective words) took 0.4s, 821671 effective words/s\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:46: EPOCH - 8 : training on 380879 raw words (348981 effective words) took 0.4s, 820853 effective words/s\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:46: EPOCH - 9 : training on 380879 raw words (348884 effective words) took 0.4s, 840125 effective words/s\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:03:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:03:47: EPOCH - 10 : training on 380879 raw words (349073 effective words) took 0.4s, 851046 effective words/s\n",
      "INFO - 12:03:47: Word2Vec lifecycle event {'msg': 'training on 3808790 raw words (3489605 effective words) took 4.2s, 824457 effective words/s', 'datetime': '2021-07-09T12:03:47.228332', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'train'}\n",
      "INFO - 12:03:47: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=16502, vector_size=150, alpha=0.025)', 'datetime': '2021-07-09T12:03:47.228642', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "\n",
    "model = gensim.models.Word2Vec(all_abs_tokens_phrases,\n",
    "        vector_size=150,\n",
    "        window=5,\n",
    "        min_count=0,\n",
    "        workers=10,\n",
    "        epochs=10,\n",
    "        sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 3: Saving the models for further use </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:04:43: Word2Vec lifecycle event {'fname_or_handle': '/home/aindani/CDSS/Metamap_eval_final/corpus_word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-07-09T12:04:43.019245', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'saving'}\n",
      "INFO - 12:04:43: not storing attribute cum_table\n",
      "INFO - 12:04:43: saved /home/aindani/CDSS/Metamap_eval_final/corpus_word2vec.model\n",
      "INFO - 12:04:43: FrozenPhrases lifecycle event {'fname_or_handle': '/home/aindani/CDSS/Metamap_eval_final/my_phrase_model.pkl', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-07-09T12:04:43.055553', 'gensim': '4.0.1', 'python': '3.9.0 (default, Nov 15 2020, 14:28:56) \\n[GCC 7.3.0]', 'platform': 'Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-glibc2.28', 'event': 'saving'}\n",
      "INFO - 12:04:43: saved /home/aindani/CDSS/Metamap_eval_final/my_phrase_model.pkl\n"
     ]
    }
   ],
   "source": [
    "model.save('corpus_word2vec.model')\n",
    "#frozen_phrases_model = ans.freeze()\n",
    "ans.save(\"my_phrase_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Get the gold standard</h3>\n",
    "Here gold standard can be in file as well. In our case we have stored everything in PostgreSQL so we are retrieving data from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdss",
   "language": "python",
   "name": "cdss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
