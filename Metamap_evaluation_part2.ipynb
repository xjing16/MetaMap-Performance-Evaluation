{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h1>Metamap Evaluation tool V1 - part 2</h1></b>\n",
    "This tool will help to evaluate and compare different metamap behavioural options.\n",
    "Part 2- This will have the code of lading the trained phrase and word2vec model, read the metamap processed files and gold standard, and finally run evaluation on different metamap options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 0 - Load database parameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "class database_params:\n",
    "    def __init__(self):\n",
    "        self.filename='/zfs/dzrptlab/CDSS/config/database.ini'\n",
    "        self.section='postgresql'\n",
    "        \n",
    "    def config(self):\n",
    "        # create a parser\n",
    "        parser = ConfigParser()\n",
    "        # read config file\n",
    "        parser.read(self.filename)\n",
    "\n",
    "        # get section, default to postgresql\n",
    "        db = {}\n",
    "\n",
    "        # Checks to see if section (postgresql) parser exists\n",
    "        if parser.has_section(self.section):\n",
    "            params = parser.items(self.section)\n",
    "            for param in params:\n",
    "                db[param[0]] = param[1]\n",
    "\n",
    "        # Returns an error if a parameter is called that is not listed in the initialization file\n",
    "        else:\n",
    "            raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    "\n",
    "        return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Get the gold standard papers</h3>\n",
    "Here gold standard can be in file as well. In our case we have stored everything in PostgreSQL so we are retrieving data from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "class get_paper_ids:\n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "\n",
    "    def get_dump(self):\n",
    "        sql = 'select distinct primary_paper_id from gold_standard_terms'\n",
    "        cursor = self.conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(sql)\n",
    "            gs_terms = cursor.fetchall()\n",
    "\n",
    "        except (Exception, psycopg2.Error) as error:\n",
    "            print(\"Error while fetching data from PostgreSQL\", error)\n",
    "\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "        return gs_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "FATAL:  could not open file \"global/pg_filenode.map\": Stale file handle\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2193f3c146df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatabase_conn_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Connect to the PostgreSQL database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_current_gs_dump_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_paper_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cdss/lib/python3.9/site-packages/psycopg2/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: FATAL:  could not open file \"global/pg_filenode.map\": Stale file handle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "database_conn_obj = database_params()\n",
    "# Obtain the configuration parameters\n",
    "params = database_conn_obj.config()\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(**params)\n",
    "\n",
    "get_current_gs_dump_obj=get_paper_ids(conn)\n",
    "paper_ids = get_current_gs_dump_obj.get_dump()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids_list=[]\n",
    "for paper in paper_ids:\n",
    "    paper_ids_list.append(str(paper[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 2: Load the trained model from part 1</h3>\n",
    "Phrase and word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class load_models:\n",
    "    def __init__(self, word2vec_path='corpus_word2vec.model', phrase_path='my_phrase_model.pkl'):\n",
    "        self.word2vec_path=word2vec_path\n",
    "        self.phrase_path=phrase_path\n",
    "    \n",
    "    def load(self):\n",
    "        phrase_model = Phrases.load(self.phrase_path)\n",
    "        w2v_model = Word2Vec.load(self.word2vec_path)\n",
    "        \n",
    "        return w2v_model, phrase_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_models_obj=load_models()\n",
    "w2v_model, phrase_model = load_models_obj.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 3: Text preprocessing </h3>\n",
    "Just like part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "import json\n",
    "\n",
    "class text_preprocessing:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "        # Choose model accordingly for contractions function\n",
    "        self.model = api.load(\"glove-twitter-25\")\n",
    "        # model = api.load(\"glove-twitter-100\")\n",
    "        # model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "        self.cont = Contractions(kv_model=self.model)\n",
    "        self.cont.load_models()\n",
    "\n",
    "        # exclude words from spacy stopwords list\n",
    "        deselect_stop_words = ['no', 'not']\n",
    "        for w in deselect_stop_words:\n",
    "            self.nlp.vocab[w].is_stop = False\n",
    "    \n",
    "\n",
    "\n",
    "    def remove_whitespace(self,text):\n",
    "        \"\"\"remove extra whitespaces from text\"\"\"\n",
    "        text = text.strip()\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "\n",
    "    def remove_accented_chars(self,text):\n",
    "        \"\"\"remove accented characters from text, e.g. cafÃ© --> cafe\"\"\"\n",
    "        text = unidecode.unidecode(text)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def expand_contractions(self,text):\n",
    "        \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
    "        text = list(self.cont.expand_texts([text], precise=True))[0]\n",
    "        return text\n",
    "\n",
    "\n",
    "    def call_preprocessing(self, text, accented_chars=True, contractions=True, \n",
    "                           convert_num=True, extra_whitespace=True, \n",
    "                           lemmatization=True, lowercase=True, punctuations=True, \n",
    "                           remove_num=True, special_chars=True, \n",
    "                           stop_words=True):\n",
    "        \"\"\"preprocess text with default option set to true for all steps\"\"\"\n",
    "\n",
    "        if extra_whitespace == True: #remove extra whitespaces\n",
    "            text = self.remove_whitespace(text)\n",
    "        if accented_chars == True: #remove accented characters\n",
    "            text = self.remove_accented_chars(text)\n",
    "        if contractions == True: #expand contractions\n",
    "            text = self.expand_contractions(text)\n",
    "        if lowercase == True: #convert all characters to lowercase\n",
    "            text = text.lower()\n",
    "\n",
    "        doc = self.nlp(text) #tokenise text\n",
    "        #print(doc)\n",
    "        clean_text = []\n",
    "\n",
    "        #remove_unwanted_dict={'objective'}\n",
    "\n",
    "        for token in doc:\n",
    "            #print(token)\n",
    "            flag = True\n",
    "            edit = token.text\n",
    "            '''\n",
    "            #remove unwanted like objective, methods\n",
    "            if token.text in remove_unwanted_dict:\n",
    "                flag = False\n",
    "            '''\n",
    "\n",
    "            # remove stop words\n",
    "            if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n",
    "                flag = False\n",
    "            # remove punctuations\n",
    "            if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n",
    "                flag = False\n",
    "            # remove special characters\n",
    "            if special_chars == True and token.pos_ == 'SYM' and flag == True: \n",
    "                flag = False\n",
    "            # remove numbers\n",
    "            if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) and flag == True:\n",
    "                flag = False\n",
    "            # convert number words to numeric numbers\n",
    "            if convert_num == True and token.pos_ == 'NUM' and flag == True:\n",
    "                edit = w2n.word_to_num(token.text)\n",
    "            # convert tokens to base form\n",
    "            elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n",
    "                edit = token.lemma_\n",
    "            # append tokens edited and not removed to list \n",
    "            if edit != \"\" and flag == True:\n",
    "                clean_text.append(edit)        \n",
    "        return clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 4: Defining Metamap parser </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class parse_metamap_json_new:\n",
    "    def __init__(self,text_preprocessing_obj):\n",
    "        self.text_preprocessing_obj = text_preprocessing_obj\n",
    "        pass\n",
    "\n",
    "    '''\n",
    "    This function returns the list of preferred name for the candidates for the input file.\n",
    "    The input file should be in json format wiz is output of metamap.\n",
    "\n",
    "    nested levels:\n",
    "\n",
    "    -AllDocuments\n",
    "    --Document\n",
    "    ---Utterances\n",
    "    ----Phrases\n",
    "    -----Mappings\n",
    "    ------MappingCandidates\n",
    "    -------CandidatePreferred\n",
    "\n",
    "    '''\n",
    "    def get_CandidatePreferred(self,file):\n",
    "\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        final_CandidateMatched_dict={}\n",
    "\n",
    "        for document in data['AllDocuments']:\n",
    "            pmid=0\n",
    "            inner_CandidateMatched={}\n",
    "            for utterance in document['Document']['Utterances']:\n",
    "                pmid=utterance['PMID']\n",
    "                for phrase in utterance['Phrases']:\n",
    "                    for mapping in phrase['Mappings']:\n",
    "                        for candidate in mapping['MappingCandidates']:\n",
    "                            CandidateMatched = ' '.join(self.text_preprocessing_obj.call_preprocessing(candidate['CandidateMatched']))\n",
    "                            if CandidateMatched == '':\n",
    "                                continue\n",
    "                            CandidatePreferred = candidate['CandidatePreferred']\n",
    "                            if CandidateMatched in inner_CandidateMatched:\n",
    "                                cur_dict=inner_CandidateMatched[CandidateMatched]\n",
    "                                if CandidatePreferred not in cur_dict:\n",
    "                                    cur_dict[CandidatePreferred] = [candidate['CandidateCUI'], candidate['CandidateScore']]\n",
    "                            else:\n",
    "                                inner_CandidateMatched[CandidateMatched]={CandidatePreferred : [candidate['CandidateCUI'], candidate['CandidateScore']]}\n",
    "                            \n",
    "            final_CandidateMatched_dict[pmid]=inner_CandidateMatched\n",
    "        return final_CandidateMatched_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 5: Gold standard dump </h3>\n",
    "This will help us to get all gold standard terms from the database\n",
    "<ol>\n",
    "    <li>define get gold standard dump class</li>\n",
    "    <li>get the dump</li>\n",
    "    <li>preprocess gold standard using text preprocessing class</li>\n",
    "    <li>form phrases using loaded phrase model</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "class get_gs_dump:\n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "\n",
    "    def get_dump(self):\n",
    "        sql = 'select primary_paper_id,gs_term from gold_standard_terms'\n",
    "        cursor = self.conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(sql)\n",
    "            gs_terms = cursor.fetchall()\n",
    "\n",
    "        except (Exception, psycopg2.Error) as error:\n",
    "            print(\"Error while fetching data from PostgreSQL\", error)\n",
    "\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "        return gs_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dump\n",
    "database_conn_obj = database_params()\n",
    "# Obtain the configuration parameters\n",
    "params = database_conn_obj.config()\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(**params)\n",
    "\n",
    "\n",
    "get_gs_dump_obj=get_gs_dump(conn)\n",
    "gs_dump = get_gs_dump_obj.get_dump()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\n",
    "gs_dump_dict={}\n",
    "for item in gs_dump:\n",
    "    if str(item[0]) in gs_dump_dict:\n",
    "        gs_dump_dict[str(item[0])].append(item[1])\n",
    "    else:\n",
    "        gs_dump_dict[str(item[0])]= [item[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess gold standard\n",
    "preprocessed_gold_standard={}\n",
    "\n",
    "for paper_id, gs_list in gs_dump_dict.items():\n",
    "    inner_list=[]\n",
    "    for term in gs_list:\n",
    "        clean=text_preprocessing_obj.call_preprocessing(term)\n",
    "        #print(str(term) +'-----'+ str(clean))\n",
    "        if len(clean) >0:\n",
    "            inner_list.append(clean)\n",
    "    if len(inner_list)>0:\n",
    "        preprocessed_gold_standard[paper_id]=inner_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass it through phrases model\n",
    "gold_standard_phrases={}\n",
    "\n",
    "for pmid, terms in preprocessed_gold_standard.items():\n",
    "    inner_phrases=[]\n",
    "    for term in terms:\n",
    "        inner_phrases.append(phrase_model[term])\n",
    "    gold_standard_phrases[pmid]=inner_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique gold standard\n",
    "\n",
    "unique_gold_standard={}\n",
    "for pmid,terms in gold_standard_phrases.items():\n",
    "    unique_inner_dict={}\n",
    "    unique_inner_list=[]\n",
    "    for term in terms:\n",
    "        unique_inner_dict[' '.join(term)]=1\n",
    "    \n",
    "    for k,v in unique_inner_dict.items():\n",
    "        unique_inner_list.append(k.split(' '))\n",
    "    unique_gold_standard[pmid]=unique_inner_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 6: Get mean vector class </h3>\n",
    "Supporting class for calculating vectors of term by taking the mean of all word vectros present in that term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class get_mean_vector_class:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_mean_vector(self, word2vec_model, words, dims=(150,)):\n",
    "        # remove out-of-vocabulary words\n",
    "        #In gensin 3.8, words = [word for word in words if word in word2vec_model.wv.vocab]\n",
    "        for word in words:\n",
    "            if word in word2vec_model.wv.key_to_index:\n",
    "                continue\n",
    "            else:\n",
    "                return \"vocab not present\"\n",
    "\n",
    "        if len(words) >= 1:\n",
    "            dummy_zeros = np.zeros(dims)\n",
    "            for word in words:\n",
    "                dummy_zeros = dummy_zeros + w2v_model.wv.get_vector(word)\n",
    "            return dummy_zeros/len(words)\n",
    "        else:\n",
    "            return \"nothing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 7: Read all the metamap processed files</h3> \n",
    "This step includes reading metamap processed files, and passing the text through text preprocessing and phrase model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of all files: use your directory\n",
    "import glob\n",
    "metamap_out_files=glob.glob('/zfs/dzrptlab/CDSS/data/metamap_data/metamap_different_options/test_5_27_41_abs/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiprocessing class for preprocessing and forming phrases of all the metamap files text\n",
    "\n",
    "import glob\n",
    "from multiprocessing import Pool, Manager\n",
    "import itertools\n",
    "\n",
    "class preprocess_and_phrases:\n",
    "    def __init__(self, metamap_out_files = glob.glob('/zfs/dzrptlab/CDSS/data/metamap_data/metamap_different_options/test_5_27_41_abs/*'), pool_size=5):\n",
    "        self.metamap_out_files=metamap_out_files\n",
    "        self.pool_size=pool_size\n",
    "        #self.metamap_different_options={}\n",
    "    \n",
    "    def f(self,file, results):\n",
    "        option_name = file.split('/')[-1]\n",
    "        print('\\n'+option_name)\n",
    "        #CandidateMatched_dict=get_CandidatePreferred(file)\n",
    "\n",
    "        #preprocess\n",
    "        preprocessed_CandidateMatched_dict={}\n",
    "        parsed = parse_metamap_json_new_obj.get_CandidatePreferred(file)\n",
    "        \n",
    "        #phrases\n",
    "        preprocessed_CandidateMatched_phrases={}\n",
    "        for pmid, candidates in parsed.items():\n",
    "            phrases_inner_list=[]\n",
    "            for candidate in candidates:\n",
    "                phrases_inner_list.append(phrase_model[candidate.split(' ')])\n",
    "\n",
    "            preprocessed_CandidateMatched_phrases[pmid]=phrases_inner_list\n",
    "           \n",
    "        results[option_name]=preprocessed_CandidateMatched_phrases\n",
    "            \n",
    "    def func(self, a_b):\n",
    "        return self.f(*a_b)\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        manager = Manager()\n",
    "        results = manager.dict()\n",
    "        \n",
    "        p = Pool(processes = self.pool_size)\n",
    "        p.map(self.func, zip(self.metamap_out_files, itertools.repeat(results)))\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your own pool size\n",
    "preprocess_and_phrases_obj=preprocess_and_phrases(pool_size=10)\n",
    "metamap_different_options = preprocess_and_phrases_obj.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 8: Define Similarity function </h3>\n",
    "Function for cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calc_cosine_similarity_class:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_cosine_similarity(self, vA, vB):\n",
    "        return np.dot(vA, vB) / (np.sqrt(np.dot(vA,vA)) * np.sqrt(np.dot(vB,vB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the objects\n",
    "get_mean_vector_class_obj = get_mean_vector_class()\n",
    "calc_cosine_similarity_class_obj = calc_cosine_similarity_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 9: Run evaluation </h3>\n",
    "This will have different functions required to do the evaluation between metamap files and gold standard.\n",
    "In the end it will print the results on the screen as well as generate a csv files with option name and results in percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "\n",
    "#1-n mappings\n",
    "def evaluate_results_top_n(metamap_different_options, gold_standard_phrases, top_n):\n",
    "    final_dict={}\n",
    "    for option_name, documents in metamap_different_options.items():\n",
    "        print('\\nMetamap option: ' + option_name)\n",
    "\n",
    "        document_dict={}\n",
    "        for pmid, matched_candidates in documents.items():\n",
    "            #take gold_standard for particular pmid\n",
    "            gold_standard = gold_standard_phrases[pmid]\n",
    "            #print(gold_standard)\n",
    "            terms_cosine_score_dict={}\n",
    "            #loop over all  matched_candidates for particular pmid\n",
    "            for candidate in matched_candidates:\n",
    "                term_vector = get_mean_vector_class_obj.get_mean_vector(w2v_model, candidate)\n",
    "                if str(term_vector) == 'vocab not present' or str(term_vector) == 'nothing':\n",
    "                    #print(str(term_vector))\n",
    "                    continue\n",
    "                #loop over all gold_standard for same pmid\n",
    "                for gold in gold_standard:\n",
    "                    GS_mean_vector = get_mean_vector_class_obj.get_mean_vector(w2v_model, gold)\n",
    "                    if str(GS_mean_vector) == 'vocab not present' or str(term_vector) == 'nothing':\n",
    "                        #print(str(gold) + ' -- GS vocab not present' )\n",
    "                        continue\n",
    "                    cos_sim_score=calc_cosine_similarity_class_obj.calc_cosine_similarity(term_vector, GS_mean_vector)\n",
    "                    if ' '.join(candidate) in terms_cosine_score_dict:\n",
    "                        #print(terms_cosine_score_dict[' '.join(candidate)])\n",
    "                        \n",
    "                        if len(terms_cosine_score_dict[' '.join(candidate)][0]) < top_n:\n",
    "                            cur_list_of_scores = terms_cosine_score_dict[' '.join(candidate)][0]\n",
    "                            insertion_point = bisect.bisect(cur_list_of_scores, cos_sim_score)\n",
    "                            bisect.insort(cur_list_of_scores, cos_sim_score)\n",
    "                            terms_cosine_score_dict[' '.join(candidate)][0] = cur_list_of_scores\n",
    "                            #updating gold standard\n",
    "                            cur_list_of_gold_standard = terms_cosine_score_dict[' '.join(candidate)][1]\n",
    "                            cur_list_of_gold_standard.insert(insertion_point, ' '.join(gold))                  \n",
    "                            terms_cosine_score_dict[' '.join(candidate)][1] = cur_list_of_gold_standard\n",
    "                            \n",
    "                            #print(cur_list_of_gold_standard,updated_gold_standard)\n",
    "                        \n",
    "                        else:\n",
    "                            cur_list_of_scores = terms_cosine_score_dict[' '.join(candidate)][0]\n",
    "                            if cos_sim_score > cur_list_of_scores[0]:\n",
    "                                insertion_point = bisect.bisect(cur_list_of_scores, cos_sim_score)\n",
    "                                bisect.insort(cur_list_of_scores, cos_sim_score)\n",
    "                                terms_cosine_score_dict[' '.join(candidate)][0] = cur_list_of_scores[-top_n:]\n",
    "                                #updating gold_standard\n",
    "                                cur_list_of_gold_standard = terms_cosine_score_dict[' '.join(candidate)][1]\n",
    "                                cur_list_of_gold_standard.insert(insertion_point, ' '.join(gold))\n",
    "                                terms_cosine_score_dict[' '.join(candidate)][1] = cur_list_of_gold_standard[-top_n:]\n",
    "                            else:\n",
    "                                continue\n",
    "        \n",
    "                    else:\n",
    "                        terms_cosine_score_dict[' '.join(candidate)] = [[cos_sim_score], [ ' '.join(gold)]]\n",
    "                    \n",
    "            document_dict[pmid] = terms_cosine_score_dict\n",
    "            \n",
    "            #calculating different metrics\n",
    "            '''\n",
    "            #print(terms_cosine_score_dict)\n",
    "            exact_per, similar_per, missing_gold = calc_eval_metrics(terms_cosine_score_dict, len(matched_candidates) * top_n, gold_standard)\n",
    "            print('For PMID ' + pmid + ' ---- exact % = ' + str(exact_per)+ ', similar % = ' + str(similar_per) + ', missing gold terms (TN) % = '+ str(round(len(missing_gold)/len(gold_standard),2 )) )               \n",
    "            print(missing_gold)\n",
    "            overall_exact += exact_per\n",
    "            overall_similar += similar_per\n",
    "            overall_missing_gold += round(len(missing_gold)/len(gold_standard),2 )\n",
    "            '''\n",
    "            \n",
    "        final_dict[option_name] = document_dict\n",
    "        #print('Overall ---- exact % =' + str(round(overall_exact/len(documents),2)) + ', similar % = ' + str(round(overall_similar/len(documents),2)) + ', missing gold terms (TN) % = '+ str(round(overall_missing_gold/len(documents),2)) )\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dict=evaluate_results_top_n(metamap_different_options, unique_gold_standard, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_eval_metrics_by_gs_top_n(input_dict, total_mappings, gold_set):\n",
    "    exact=0\n",
    "    similar=0\n",
    "    different=0\n",
    "    exact_set={}\n",
    "    similar_set={}\n",
    "    \n",
    "    exact_set_by_gs=set()\n",
    "    similar_set_by_gs=set()\n",
    "    missing_gs_set_by_gs=set()\n",
    "    \n",
    "    \n",
    "    for k,v in input_dict.items():\n",
    "        for pair_index in range(len(v[0])):\n",
    "            if v[0][pair_index] >= .85:\n",
    "                exact +=1\n",
    "                exact_set[k]=[v[1][pair_index], round(v[0][pair_index],2)]\n",
    "                exact_set_by_gs.add(v[1][pair_index])\n",
    "                gold_set.discard(v[1][pair_index])\n",
    "            elif v[0][pair_index] > 0.65 and v[0][pair_index] < 0.85:\n",
    "                similar += 1\n",
    "                similar_set[k]=[v[1][pair_index], round(v[0][pair_index],2)]\n",
    "                similar_set_by_gs.add(v[1][pair_index])\n",
    "                gold_set.discard(v[1][pair_index])\n",
    "            else:\n",
    "                different +=1\n",
    "                missing_gs_set_by_gs.add(v[1][pair_index])\n",
    "                \n",
    "    \n",
    "    return round(exact/total_mappings,2), round(similar/total_mappings,2), gold_set, exact_set, similar_set, exact_set_by_gs, similar_set_by_gs, missing_gs_set_by_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def run_eval_metrics_by_gs_top_n(input_dict, top_n, gold_standard_phrases, myfile, out_csv):\n",
    "    options=[]\n",
    "    overall_exact_by_gs_list=[]\n",
    "    overall_similar_by_gs_list=[]\n",
    "    overall_missing_by_gs_list=[]\n",
    "    \n",
    "    for option_name, documents in input_dict.items():\n",
    "        options.append(option_name)\n",
    "        print('\\nMetamap option: ' + option_name)\n",
    "        myfile.write('\\nMetamap option: %s\\n'%option_name)\n",
    "        overall_exact=0\n",
    "        overall_similar=0\n",
    "        overall_missing_gold=0\n",
    "        \n",
    "        overall_exact_by_gs=0\n",
    "        overall_similar_by_gs=0\n",
    "        overall_gold = 0\n",
    "        overall_missing_gold_by_gs=0\n",
    "        \n",
    "        for pmid, candidates in documents.items():\n",
    "            gold_standard = gold_standard_phrases[pmid]\n",
    "            \n",
    "            gold_set=set()\n",
    "            for gold in gold_standard:\n",
    "                gold_set.add(' '.join(gold))\n",
    "                        \n",
    "            exact_per, similar_per, missing_gold, exact_set, similar_set, exact_set_by_gs, similar_set_by_gs, missing_gs_set_by_gs = calc_eval_metrics_by_gs_top_n(candidates, len(candidates) * top_n, gold_set.copy())\n",
    "           \n",
    "            print('\\nFor PMID ' + pmid + ' ---- Exact matches by gs: '+str(round(len(exact_set_by_gs)/len(gold_set),2)) + ', Similar matches by gs: '+str(round(len(similar_set_by_gs.difference(exact_set_by_gs))/len(gold_set),2)) + ', missing gold terms (TN): '+ str(round(len(missing_gold)/len(gold_set),2 )))\n",
    "            myfile.write('\\nFor PMID ' + pmid + ' ---- Exact matches by gs: '+str(round(len(exact_set_by_gs)/len(gold_set),2)) + ', Similar matches by gs: '+str(round(len(similar_set_by_gs.difference(exact_set_by_gs))/len(gold_set),2)) + ', missing gold terms (TN): '+ str(round(len(missing_gold)/len(gold_set),2 )))\n",
    "            myfile.write('\\nExact matched metamap to gold standard pairs: ' +str(exact_set))\n",
    "            myfile.write('\\nSimilar matched metamap to gold standard pairs: ' +str(similar_set))\n",
    "            myfile.write('\\nMissing gold standard: '+str(missing_gold))\n",
    "\n",
    "            overall_exact += exact_per\n",
    "            overall_similar += similar_per\n",
    "            overall_missing_gold += round(len(missing_gold)/len(gold_set),2 )\n",
    "            \n",
    "            overall_exact_by_gs += len(exact_set_by_gs)\n",
    "            overall_similar_by_gs += len(similar_set_by_gs.difference(exact_set_by_gs))\n",
    "            overall_missing_gold_by_gs += len(missing_gold)\n",
    "            overall_gold += len(gold_set)\n",
    "            #print(overall_gold)\n",
    "        \n",
    "        #print('Overall ---- exact % =' + str(round(overall_exact/len(documents),2)) + ', similar % = ' + str(round(overall_similar/len(documents),2)) + ', missing gold terms (TN) % = '+ str(round(overall_missing_gold/len(documents),2)) )\n",
    "        print('Overall ---- exact by gs % =' + str(round(overall_exact_by_gs/overall_gold,2)) + ', similar by gs % = ' + str(round(overall_similar_by_gs/overall_gold,2)) + ', missing gold terms (TN) % = '+ str(round((overall_missing_gold_by_gs)/overall_gold,2)))\n",
    "        myfile.write('Overall ---- exact by gs % =' + str(round(overall_exact_by_gs/overall_gold,2)) + ', similar by gs % = ' + str(round(overall_similar_by_gs/overall_gold,2)) + ', missing gold terms (TN) % = '+ str(round((overall_missing_gold_by_gs)/overall_gold,2)))\n",
    "        overall_exact_by_gs_list.append(round(overall_exact_by_gs/overall_gold,2))\n",
    "        overall_similar_by_gs_list.append(round(overall_similar_by_gs/overall_gold,2))\n",
    "        overall_missing_by_gs_list.append(round((overall_missing_gold_by_gs)/overall_gold,2))\n",
    "        \n",
    "        #myfile.write('\\nOverall ---- exact % =' + str(round(overall_exact/len(documents),2)) + ', similar % = ' + str(round(overall_similar/len(documents),2)) + ', missing gold terms (TN) % = '+ str(round(overall_missing_gold/len(documents),2)) +'\\n')\n",
    "        #myfile.write('Overall ---- exact by gs % =' + str(round(overall_exact_by_gs/overall_gold,2)) + ', similar by gs % = ' + str(round(overall_similar_by_gs/overall_gold,2 )))\n",
    "    df=pd.DataFrame()\n",
    "    df['options']=options\n",
    "    df['overall_exact_by_gs_list']=overall_exact_by_gs_list\n",
    "    df['overall_similar_by_gs_list'] = overall_similar_by_gs_list\n",
    "    df['overall_missing_by_gs_list'] = overall_missing_by_gs_list\n",
    "    \n",
    "    df.to_csv(out_csv,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open('2_option_top_3_cosine_by_gs.log', 'w')\n",
    "run_eval_metrics_by_gs_top_n(answer_dict, 3, gold_standard_phrases, myfile, '2_option_top_3_cosine_by_gs_results.csv')\n",
    "myfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdss",
   "language": "python",
   "name": "cdss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
